import streamlit as st
import os
import re
import json
import datetime
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS

# --- 1. C·∫§U H√åNH TRANG (SIDEBAR C·ªê ƒê·ªäNH) ---
st.set_page_config(
    page_title="Yoga Guru AI",
    page_icon="üßò",
    layout="wide",
    initial_sidebar_state="expanded"  # <-- C·ªë ƒë·ªãnh Sidebar lu√¥n m·ªü
)

# --- 2. GIAO DI·ªÜN ƒê·∫∏P (CSS) ---
st.markdown("""
<style>
    /* Ch·ªânh font ch·ªØ v√† m√†u n·ªÅn chung */
    .stApp {background-color: #f8f9fa;}
    
    /* Tin nh·∫Øn c·ªßa AI: N·ªÅn tr·∫Øng, bo tr√≤n */
    div[data-testid="stChatMessage"] {
        background-color: #ffffff;
        border-radius: 15px;
        padding: 15px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        margin-bottom: 10px;
    }
    
    /* Tin nh·∫Øn c·ªßa User: N·ªÅn xanh nh·∫°t */
    div[data-testid="stChatMessage"][data-test-role="user"] {
        background-color: #e3f2fd;
        flex-direction: row-reverse; /* ƒê·∫£o chi·ªÅu avatar */
        text-align: right;
    }

    /* Sidebar ƒë·∫πp h∆°n */
    section[data-testid="stSidebar"] {
        background-color: #ffffff;
        border-right: 1px solid #ddd;
    }
    
    /* Link m√†u cam th∆∞∆°ng hi·ªáu */
    .stMarkdown a {color: #ff6b6b !important; font-weight: bold;}
    
    /* ·∫®n footer m·∫∑c ƒë·ªãnh */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# --- 3. C·∫§U H√åNH API ---
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
except:
    st.error("‚ö†Ô∏è L·ªói: Ch∆∞a c·∫•u h√¨nh API Key trong Secrets.")
    st.stop()

VECTOR_DB_PATH = "bo_nao_vector"
USAGE_DB_FILE = "usage_database.json"
DAILY_LIMIT = 15
TRIAL_LIMIT = 5

# --- QU·∫¢N L√ù QUOTA ---
def load_usage_db():
    if not os.path.exists(USAGE_DB_FILE): return {}
    with open(USAGE_DB_FILE, "r") as f: return json.load(f)

def save_usage_db(data):
    with open(USAGE_DB_FILE, "w") as f: json.dump(data, f)

def check_member_limit(username):
    data = load_usage_db()
    today = str(datetime.date.today())
    if username not in data or data[username]["date"] != today:
        data[username] = {"date": today, "count": 0}
        save_usage_db(data)
        return 0, DAILY_LIMIT
    used = data[username]["count"]
    return used, DAILY_LIMIT - used

def increment_member_usage(username):
    data = load_usage_db()
    today = str(datetime.date.today())
    if username in data and data[username]["date"] == today:
        data[username]["count"] += 1
        save_usage_db(data)

# --- X·ª¨ L√ù T·ª™ KH√ìA ---
SPECIAL_MAPPING = {
    "tr·ªìng chu·ªëi": ["sirsasana", "headstand", "ƒë·ª©ng b·∫±ng ƒë·∫ßu"],
    "con qu·∫°": ["bakasana", "crow"],
    "c√°i c√†y": ["halasana", "plow"],
    "tam gi√°c": ["trikonasana", "triangle"],
    "x√°c ch·∫øt": ["savasana", "corpse"],
    "b√°nh xe": ["chakrasana", "wheel"],
    "ch√≥ √∫p m·∫∑t": ["adho mukha svanasana", "downward facing dog"],
    "r·∫Øn h·ªï mang": ["bhujangasana", "cobra"]
}
STOPWORDS = {'l√†', 'c·ªßa', 'nh·ªØng', 'c√°i', 'vi·ªác', 'trong', 'khi', 'b·ªã', 'v·ªõi', 'cho', 'ƒë∆∞·ª£c', 't·∫°i', 'v√¨', 'sao', 'th√¨', 'l·∫°i', 'm√†', 'v√†', 'c√°c', 'c√≥', 'nh∆∞', 'ƒë·ªÉ', 'n√†y', 'ƒë√≥', 'v·ªÅ', 'theo', 'nh·∫•t', 'g√¨', 'th·∫ø', 'n√†o', 'l√†m', 't·∫≠p', 'b√†i', 'c√°ch', 'nh∆∞', 'th·∫ø', 'n√†o', 't√¥i', 'b·∫°n', 'mu·ªën', 'h·ªèi'}

def clean_and_extract_keywords(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    words = text.split()
    return set([w for w in words if w not in STOPWORDS and len(w) > 1])

# --- 4. LOAD BRAIN (C√ì DEBUG L·ªñI) ---
@st.cache_resource
def load_brain():
    # Ki·ªÉm tra xem folder c√≥ t·ªìn t·∫°i kh√¥ng
    if not os.path.exists(VECTOR_DB_PATH):
        st.error(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c '{VECTOR_DB_PATH}'. B·∫°n h√£y ki·ªÉm tra l·∫°i tr√™n GitHub xem ƒë√£ upload folder n√†y l√™n ch∆∞a.")
        return None, None
    
    # Ki·ªÉm tra file index c√≥ b·ªã l·ªói Git LFS kh√¥ng (N·∫øu file qu√° nh·∫π < 10KB l√† l·ªói)
    index_file = os.path.join(VECTOR_DB_PATH, "index.faiss")
    if os.path.exists(index_file):
        file_size = os.path.getsize(index_file)
        if file_size < 10000: # Nh·ªè h∆°n 10KB
            st.error(f"‚ùå L·ªñI NGHI√äM TR·ªåNG: File d·ªØ li·ªáu qu√° nh·∫π ({file_size} bytes). ƒê√¢y l√† l·ªói do Git LFS ch∆∞a t·∫£i ƒë∆∞·ª£c file g·ªëc l√™n. Vui l√≤ng xem l·∫°i b∆∞·ªõc upload Git LFS.")
            return None, None

    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)
    try:
        db = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        model = genai.GenerativeModel('gemini-1.5-flash-latest') 
        return db, model
    except Exception as e:
        st.error(f"‚ùå L·ªói khi n·∫°p d·ªØ li·ªáu: {e}")
        return None, None

db, model = load_brain()

def search_engine(query, db):
    query_lower = query.lower()
    user_keywords = clean_and_extract_keywords(query)
    injected_keywords = set()
    for key, values in SPECIAL_MAPPING.items():
        if key in query_lower:
            injected_keywords.update(values)
            user_keywords.update(values)
    if not user_keywords: user_keywords = set(query_lower.split())
    vector_query = f"{query} {' '.join(injected_keywords)}"
    raw_docs = db.similarity_search(vector_query, k=200)
    matched_docs = []
    for d in raw_docs:
        title = d.metadata.get('title', 'No Title')
        content = d.page_content
        title_keywords = clean_and_extract_keywords(title)
        score = 0
        common_words = user_keywords.intersection(title_keywords)
        if len(common_words) > 0:
            score += len(common_words) * 10
            for inj in injected_keywords:
                if inj in title.lower(): score += 500
            match_ratio = len(common_words) / len(user_keywords) if len(user_keywords) > 0 else 0
            if match_ratio >= 0.5: score += 50
        if score == 0:
            content_keywords = clean_and_extract_keywords(content[:500])
            common_content = user_keywords.intersection(content_keywords)
            if len(common_content) > 0: score += len(common_content)
        if score > 0: matched_docs.append((d, score))
    matched_docs.sort(key=lambda x: x[1], reverse=True)
    return [x[0] for x in matched_docs[:6]]

# --- TR·∫†NG TH√ÅI PHI√äN ---
if "authenticated" not in st.session_state: st.session_state.authenticated = False
if "username" not in st.session_state: st.session_state.username = ""
if "guest_usage" not in st.session_state: st.session_state.guest_usage = 0
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Namaste! üôè H·ªèi m√¨nh b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ Yoga nh√©."}]

# --- SIDEBAR C·ªê ƒê·ªäNH & ƒê·∫∏P ---
with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/2647/2647596.png", width=60) # Icon Logo
    st.title("Yoga Guru AI")
    st.markdown("---")
    
    if st.session_state.authenticated:
        st.success(f"üëã Ch√†o Yogi, **{st.session_state.username}**!")
        used, remaining = check_member_limit(st.session_state.username)
        
        # Thanh progress bar ƒë·∫πp
        percent = used / DAILY_LIMIT
        st.progress(percent)
        st.write(f"üìä ƒê√£ d√πng: **{used}/{DAILY_LIMIT}** c√¢u")
        
        if st.button("üö™ ƒêƒÉng xu·∫•t", type="secondary"):
            st.session_state.authenticated = False
            st.rerun()
    else:
        st.info("üå± Ch·∫ø ƒë·ªô: **Kh√°ch d√πng th·ª≠**")
        st.metric(label="C√¢u h·ªèi c√≤n l·∫°i", value=f"{TRIAL_LIMIT - st.session_state.guest_usage}", delta=None)
        
        if st.session_state.guest_usage >= TRIAL_LIMIT:
            st.warning("üîí H·∫øt l∆∞·ª£t th·ª≠.")
            with st.form("login_form"):
                user_input = st.text_input("T√†i kho·∫£n")
                pass_input = st.text_input("M·∫≠t kh·∫©u", type="password")
                if st.form_submit_button("üîë ƒêƒÉng nh·∫≠p ngay"):
                    secrets_pass = st.secrets["passwords"].get(user_input)
                    if secrets_pass and secrets_pass == pass_input:
                        st.session_state.authenticated = True
                        st.session_state.username = user_input
                        st.rerun()
                    else: st.error("Sai m·∫≠t kh·∫©u!")
        else:
             st.markdown("---")
             with st.expander("üîê Th√†nh vi√™n ƒëƒÉng nh·∫≠p"):
                with st.form("login_form_guest"):
                    user_input = st.text_input("T√†i kho·∫£n")
                    pass_input = st.text_input("M·∫≠t kh·∫©u", type="password")
                    if st.form_submit_button("ƒêƒÉng nh·∫≠p"):
                        secrets_pass = st.secrets["passwords"].get(user_input)
                        if secrets_pass and secrets_pass == pass_input:
                            st.session_state.authenticated = True
                            st.session_state.username = user_input
                            st.rerun()
                        else: st.error("Sai th√¥ng tin!")
    
    st.markdown("---")
    st.caption("¬© 2024 Yoga Guru AI")

# --- GIAO DI·ªÜN CHAT ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- KI·ªÇM TRA QUY·ªÄN CHAT ---
can_chat = False
if st.session_state.authenticated:
    used, remaining = check_member_limit(st.session_state.username)
    if remaining > 0: can_chat = True
else:
    if st.session_state.guest_usage < TRIAL_LIMIT: can_chat = True

if can_chat:
    if prompt := st.chat_input("VD: ƒêau l∆∞ng t·∫≠p g√¨? K·ªπ thu·∫≠t con qu·∫°..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)

        with st.chat_message("assistant"):
            if db is None:
                st.error("‚ùå H·ªá th·ªëng ƒëang b·∫£o tr√¨ d·ªØ li·ªáu (L·ªói DB). Vui l√≤ng th·ª≠ l·∫°i sau.")
                st.stop()
            
            msg_placeholder = st.empty()
            msg_placeholder.markdown("üßò *Guru ƒëang suy ng·∫´m...*")
            
            try:
                top_docs = search_engine(prompt, db)
                
                if st.session_state.authenticated: increment_member_usage(st.session_state.username)
                else: st.session_state.guest_usage += 1

                if not top_docs:
                    resp = "Guru ch∆∞a t√¨m th·∫•y b√†i vi·∫øt ph√π h·ª£p trong th∆∞ vi·ªán."
                    msg_placeholder.markdown(resp)
                    st.session_state.messages.append({"role": "assistant", "content": resp})
                else:
                    context = ""
                    links = {}
                    for i, d in enumerate(top_docs):
                        title = d.metadata.get('title', 'No Title')
                        url = d.metadata.get('url', '#')
                        context += f"[B√ÄI {i+1}]: {title}\nN·ªôi dung: {d.page_content}\n\n"
                        if url != '#' and "http" in url and url not in links:
                             clean = title.replace("[", "").replace("]", "").replace("(", " - ").replace(")", "")
                             links[url] = clean
                    
                    link_md = ""
                    if links:
                        link_md = "\n\n---\n**üìö Tham kh·∫£o chi ti·∫øt:**\n" + "\n".join([f"- [{n}]({u})" for u, n in links.items()])

                    # --- PROMPT V33: ƒê·∫∏P & G·ªåN ---
                    sys_prompt = f"""
                    B·∫°n l√† Yoga Guru chuy√™n nghi·ªáp.
                    D·ª±a tr√™n d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi.
                    
                    D·ªÆ LI·ªÜU:
                    {context}
                    
                    C√ÇU H·ªéI: "{prompt}"
                    
                    Y√äU C·∫¶U:
                    1. Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng c√°c g·∫°ch ƒë·∫ßu d√≤ng (kho·∫£ng 8-10 √Ω).
                    2. T·ªïng ƒë·ªô d√†i kho·∫£ng 200 t·ª´.
                    3. VƒÉn phong chuy√™n nghi·ªáp, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.
                    4. KH√îNG t·ª± vi·∫øt link.
                    """
                    
                    response = model.generate_content(sys_prompt)
                    full_resp = response.text + link_md
                    msg_placeholder.markdown(full_resp)
                    st.session_state.messages.append({"role": "assistant", "content": full_resp})
                    st.rerun()

            except Exception as e: 
                st.error("L·ªói h·ªá th·ªëng. Vui l√≤ng th·ª≠ l·∫°i."); print(e)
else:
    if st.session_state.authenticated:
        st.info("‚õî H√¥m nay b·∫°n ƒë√£ h·ªèi ƒë·ªß 15 c√¢u r·ªìi. H·∫πn g·∫∑p l·∫°i ng√†y mai nh√©!")
    else:
        st.warning(f"üîí B·∫°n ƒë√£ h·∫øt {TRIAL_LIMIT} c√¢u h·ªèi d√πng th·ª≠. Vui l√≤ng **ƒêƒÉng nh·∫≠p** ·ªü c·ªôt b√™n tr√°i.")
